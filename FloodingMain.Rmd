---
title: "Flooding Project"
author: "Sarah Driver, Kenneth Lewis, Josh Rivera, Ryan Fisher, Emi Carbray"
date: "3/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(tidyverse) #load packages
library(ggthemes)
library(huxtable)
library(readxl)
library(stringr)
library(readr)
#install.packages("usdata")
library(usdata)
#install.packages("here")
library(here)  #Keeps working directories consistent across OSes. Put the csv files in the directory where this project is installed.
#install.packages('usmap')
library(usmap)
#HUD_data <- read.csv(file = "./Documents/PUAD-688/HUD_neighborhood_data.csv") 
HUD_data <- read_excel(here("activeportfoliopropdata.xlsx"), sheet = "Step_01_Property_Level_data")

typical_home_value_from_Zillow <- read.csv(here("County_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv")) #This is a time series of TYPICAL home values, smoothed, seasonally adjusted, for 35th to 65th percentile homes, and located at: https://www.zillow.com/research/data/. Data name in the selection part is "ZHVI All Homes (SFR, Condo/Co-op) Time-series, Smoothed, Seasonally Adjusted $."

#county_info_with_populations <- read.csv(file = "./Documents/PUAD-688/RuralAtlasData23People.csv") 

#USDA county level data from: https://www.ers.usda.gov/data-products/atlas-of-rural-and-small-town-america/download-the-data/ 
#Copy the folder "Release23_June2021" into the working directory of the flooding project
#county_info2 <- read.csv(file = here("Release23_June2021", "RuralAtlasData23.csv")) #I only found this file as xlsx, not csv, so that's what I use below. The income data goes to county_info2 while the population data goes to county_info_with_population
county_info2 <- read_excel(here("Release23_June2021", "RuralAtlasData23.xlsx"), sheet = "Income")
county_info_with_populations <- read_excel(here("Release23_June2021", "RuralAtlasData23.xlsx"), sheet = "People")
  
#Now adding info about NFIP claims for Fiscal Year 2022
NFIP_financial_losses_for_2022 <- read.csv(here("nfip_financial-losses-by-state_20220228.csv"))
#took this from https://nfipservices.floodsmart.gov/reports-flood-insurance-data 

#Average NFIP Claims Data by State and Territory 1996-2019 (taken from their website https://www.fema.gov/data-visualization/historical-flood-risk-and-costs on March 30, 2022)
Average_NFIP_claims_1996_to_2019 <- read_excel(here("Average NFIP Claims Data by State and Territory 1996-2019.xlsx"))

#Numeracy and literacy by county from https://nces.ed.gov/surveys/piaac/skillsmap/ 
abilities_by_county <- read.csv(here("SAE_website_dataset.b29ed56c.csv"))
#Lit_P1 = Literacy proportion at or below Level 1 indirect estimates
#Lit_A = Literacy average score indirect estimates
#Num_P1 = Numeracy proportion at or below Level 1 indirect estimates 
#Num_A = Numeracy average score indirect estimates

#NFIP_Reinsurance_Placement2020 <- read.csv(file = "./Documents/PUAD-688/NFIP_2021_Exposure_by_DTW(2021_NFIP_Reinsurance_Placement_Information).csv") 
#Retain roll up by number of stories. We skip the first 7 lines because these contain description, not the dataset
NFIP_Reinsurance_Placement2020 <- read_excel(here("NFIP", "NFIP_2021_Exposure_by_DTW.xlsx"), sheet= "NumStor_County_DTW", skip = 7)
#some new changes

#Looking at everything
str(HUD_data) #a lot are chr and int 
glimpse(HUD_data)
```

## Data Cleaning

```{r Cleaning}

#New data
HUD_narrowed <- HUD_data %>% select(property_id, address_line1_text, city_name_text, state_code, state_name_text, county_code, county_name_text)
#generate a FIPS code from the state_code and count_code
HUD_narrowed$FIPS <- (as.integer(fips(HUD_narrowed$state_code))*1000+as.integer(HUD_narrowed$county_code))
#drop observations that are missing data
HUD_narrowed <- HUD_narrowed %>% drop_na()

#Making a list of HUD counties 
HUD_counties <- HUD_narrowed %>% select(FIPS, county_name_text)
#HUD_county_names <- HUD_data %>% select(county_name_text, state_code)

#Check for duplicates 
#length(unique(HUD_county_names$county_name_text)) == nrow(HUD_county_names) #FALSE = there are duplicates
length(unique(HUD_counties$FIPS))
#nrow(HUD_counties)#yes, there are many duplicates
HUD_counties <- unique(HUD_counties) #drop all duplicates

#Note: US Virgin Islands are in here, might want to double check them
#TIV means total insurable value 

#Check
str(HUD_counties)
str(county_info2) #county info includes data aggregated at a higher level, e.g. FIPS code 01000 is all of Alabama

#Remove white space trailing words
HUD_counties$county_name_text <- trimws(HUD_counties$county_name_text)
county_info2$County <- trimws(county_info2$County)

#Creating fips codes for Zillow data
typical_home_value_from_Zillow$FIPS <- (as.integer(fips(typical_home_value_from_Zillow$StateName))*1000+as.integer(typical_home_value_from_Zillow$MunicipalCodeFIPS))

#Changing Zillow data's column names to be nicer
typical_home_value_from_Zillow <- typical_home_value_from_Zillow %>% rename_with(~paste0(., "_Zillow_date"), X1.31.00:X2.28.22)

#USEFUL: PCTPOVALL = Total poverty in 2019

#Change up the #literacy and numeracy data to make clearer 
abilities_by_county <- abilities_by_county %>% 
  mutate(Literacy_proportion_at_or_below_Level_1_indirect_estimates = Lit_P1 ) %>%
  select(-Lit_P1)

abilities_by_county <- abilities_by_county %>% 
  mutate(Literacy_average_score = Lit_A) %>%
  select(-Lit_A)

abilities_by_county <- abilities_by_county %>% 
  mutate(Numeracy_proportion_at_or_below_Level_1_indirect_estimates= Num_P1) %>%
  select(-Num_P1)

abilities_by_county <- abilities_by_county %>% 
  mutate(Numeracy_average_score = Num_A) %>%
  select(-Num_A)

#Now fixing up NFIP data for combination by removing one word from phrases
NFIP_data <- NFIP_Reinsurance_Placement2020 %>%
  mutate(County = str_remove_all(County, "COUNTY")) #removing all of COUNTY from county names 

#Making NFIP_financial_losses_for_2022 have clearer column titles
NFIP_financial_losses_for_2022 <- NFIP_financial_losses_for_2022 %>% rename_with(~paste0(., "_Fiscal_Year_2022"), Number.of.Records:Total.Payments)

#check out data 
str(NFIP_data)

#Remove commas from numbers and make into integer
NFIP_data$RiskCount <- as.integer((gsub(",", "", NFIP_data$RiskCount)))

#Making the name not be capitalized 
NFIP_data$County <- str_to_title(NFIP_data$County)
county_info2$County <- str_to_title(county_info2$County)
#Remove white space 
NFIP_data$County <- trimws(NFIP_data$County)
NFIP_data$State <- trimws(NFIP_data$State)

#Create column of added up counts within counties to divide by risk numbers
NFIP_data <- NFIP_data%>%
  add_count(FIPS) #n is the count

#Create column of added up risk counts 
NFIP_data <- NFIP_data%>%
  group_by(FIPS) %>% 
  mutate(Summed_Risk = sum(RiskCount))

#Get rid of FIPS
#NFIP_data <- NFIP_data %>% select(-FIPS)

#Get rid of data I don't want right now in NFIP stuff 
#NFIP_data <- NFIP_data %>% select(-DTW_band, -RiskCount, -TIV, -Limit, -RollUp_Level, -County, -State)
#NFIP_data <- NFIP_data %>%distinct()

#Join NFIP flood reinsurance info to joined dataset
county_info2$FIPS <- as.integer(county_info2$FIPS) #convert county_info2$FIPS to integer
joined_county_data <- left_join(county_info2, NFIP_data, by= "FIPS")
#checking if counties are named differently in the merged datasets
joined_county_data$badstatecounty <- joined_county_data$County.x != joined_county_data$County.y

#Join Zillow data to joined dataset
typical_home_value_from_Zillow$FIPS <- as.integer(typical_home_value_from_Zillow$FIPS) #convert typical_home_value_from_Zillow$FIPS to integer
joined_county_data <- left_join(joined_county_data, typical_home_value_from_Zillow, by= "FIPS")

#Neaten joined_county_data by removing duplicate columns and fixing column names
joined_county_data <- joined_county_data %>% select(-State.y, -County.y)

#Standardize capitalization of state names in NFIP_financial_losses_for_2022 dataset
NFIP_financial_losses_for_2022$State <- str_to_title(NFIP_financial_losses_for_2022$State)

#Make claims datasets have abbreviations 
Average_NFIP_claims_1996_to_2019$State <- state2abbr(Average_NFIP_claims_1996_to_2019$State)
NFIP_financial_losses_for_2022$State <- state2abbr(NFIP_financial_losses_for_2022$State)

#Join claims data to joined_county_data 
joined_county_data <- left_join(joined_county_data, Average_NFIP_claims_1996_to_2019, by= "State")
joined_county_data <- left_join(joined_county_data, NFIP_financial_losses_for_2022, by= c("StateName"= "State"))

#The risk categories are for flood reinsurance 

#New column for risk
joined_county_data <- joined_county_data %>% 
  mutate(Risk_by_Instances_in_County = Summed_Risk/n)

#Drop weird rows that have "US" as the state 
joined_county_data <- subset(joined_county_data, State.x != "US")

#Look at very poor counties 
poor_counties <- joined_county_data %>%
  filter(PCTPOVALL >= 20) 
```

## Write to File and Clean up
```{r WriteFile, echo=TRUE}
#Save data frames as .csv
#write_csv(poor_counties, "./Documents/PUAD-688/Poor_Counties.csv")
#write_csv(joined_county_data, "./Documents/PUAD-688/Some_Joined_Data.csv")

```


## Data Analysis

```{r analysis, echo=TRUE}
## Do something
```
